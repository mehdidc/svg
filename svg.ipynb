{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't import dot_parser, loading of dot files will not be possible.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 1: Tesla K20Xm (CNMeM is disabled)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "%matplotlib inline\n",
    "os.environ[\"THEANO_FLAGS\"] = \"device=gpu,exception_verbosity=high\"\n",
    "import theano.tensor as T\n",
    "from collections import OrderedDict\n",
    "from lasagne import init, updates , layers\n",
    "w, h = 28, 28\n",
    "import theano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mcherti/miniconda/envs/databoard-env/lib/python2.7/site-packages/PIL/Image.py:861: UserWarning: Palette images with Transparency   expressed in bytes should be converted to RGBA images\n",
      "  'to RGBA images')\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "from skimage.io import imread\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from skimage.transform import resize\n",
    "inputs = []\n",
    "outputs = []\n",
    "outputs_non_normalized = []\n",
    "x_min = []\n",
    "x_max = []\n",
    "y_min = []\n",
    "y_max = []\n",
    "for filename in glob.glob(\"png/*.png\"):\n",
    "    inp = resize(imread(filename), (w, h)).tolist()\n",
    "    inputs.append(inp)\n",
    "    name = (os.path.splitext(os.path.basename(filename))[0])\n",
    "    filename_txt = \"svg/{0}.svg.txt\".format(name)\n",
    "    df = pd.read_table(filename_txt, skiprows=1, \n",
    "                       sep=' ',\n",
    "                       names=[\"p1x\", \"p1y\", \"p2x\", \"p2y\", \"p3x\", \"p3y\", \"p4x\", \"p4y\"])\n",
    "    #df.loc[len(df)] = [-1] * 8\n",
    "    o = df.values.copy().astype(np.float32)\n",
    "    \n",
    "    outputs_non_normalized.append(o.copy())\n",
    "    \n",
    "    \n",
    "    s = slice(0, len(o))\n",
    "    m = (0, 2, 4, 6)\n",
    "    x_min.append(o[s, m].min())\n",
    "    x_max.append(o[s, m].max())\n",
    "    o[s, m] -= o[s, m].min()\n",
    "    o[s, m] /= o[s, m].max()\n",
    "    \n",
    "    m = (1, 3, 5, 7)\n",
    "    y_min.append(o[s, m].min())\n",
    "    y_max.append(o[s, m].max())\n",
    "    o[s, m] -= o[s, m].min()\n",
    "    o[s, m] /= o[s, m].max()\n",
    "    \n",
    "    #o = np.concatenate( (np.zeros((1, 8)), o), axis=0)    \n",
    "    #o =  o[1:, :] - o[0:-1, :]\n",
    "    o = np.concatenate( (np.zeros((1, 8)), o), axis=0)\n",
    "    o = np.concatenate( (o, np.zeros((1, 8))), axis=0)\n",
    "    o = o.astype(np.float32)\n",
    "    #o=o*2-1\n",
    "    outputs.append(o)\n",
    "\n",
    "inputs = np.array(inputs).astype(np.float32)\n",
    "inputs /= 255.\n",
    "inputs = 1 - inputs\n",
    "inputs = inputs.reshape((inputs.shape[0], w*h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from lasagne.layers import Layer\n",
    "class RepeatLayer(Layer):\n",
    "    \n",
    "    def __init__(self, incoming, shape, desired_shape, **kwargs):\n",
    "        super(RepeatLayer, self).__init__(incoming, **kwargs)\n",
    "        self.shape = shape\n",
    "        self.desired_shape = desired_shape\n",
    "        \n",
    "    def get_output_for(self, input, **kwargs):\n",
    "        input_ = input.dimshuffle(*self.desired_shape)\n",
    "        return input_ * T.ones(self.desired_shape)\n",
    "    \n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        return [b if a == 'x' else input_shape[a] \n",
    "                for a, b in zip(self.shape, self.desired_shape)]\n",
    "\n",
    "class StepLayer(Layer):\n",
    "    def __init__(self, incoming, iteration_layer, **kwargs):\n",
    "        super(StepLayer, self).__init__(incoming, **kwargs)\n",
    "        self.iteration_layer = iteration_layer\n",
    "    \n",
    "    def get_output_for(self, input, **kwargs):\n",
    "        \n",
    "        def step(input_i):\n",
    "            return layers.get_output(self.iteration_layer, input_i)\n",
    "        sequences = [input]\n",
    "        return theano.scan(fn=step, sequences=sequences, \n",
    "                           outputs_info=None,\n",
    "                           strict=False)[0]\n",
    "    \n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        return tuple([input_shape[0]] + list(self.iteration_layer.output_shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mcherti/work/code/scikit-learn/sklearn/cross_validation.py:42: DeprecationWarning: This module has been deprecated in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.19.\n",
      "  \"This module will be removed in 0.19.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import lasagne.layers\n",
    "from lasagne.layers import Gate\n",
    "from lasagne.init import HeUniform\n",
    "from lasagnekit.generative.capsule import Capsule\n",
    "from lasagnekit.easy import LightweightModel, BatchOptimizer\n",
    "n_in = 8\n",
    "n_hid = 800\n",
    "size_mixture = 20\n",
    "n_out = 8 * 2 * size_mixture + size_mixture\n",
    "nb_layers = 2\n",
    "l_in = lasagne.layers.InputLayer((None, None, n_in))\n",
    "l_rec = l_in\n",
    "for i in range(nb_layers):\n",
    "    l_rec = lasagne.layers.LSTMLayer(l_rec, n_hid, grad_clipping=10)\n",
    "    #l_rec = lasagne.layers.RecurrentLayer(l_rec, n_hid, \n",
    "    #                                      W_in_to_hid=HeUniform(), \n",
    "    #                                      grad_clipping=10,\n",
    "    #                                      W_hid_to_hid=HeUniform())\n",
    "\n",
    "l_out = lasagne.layers.LSTMLayer(l_rec, n_out, grad_clipping=10, \n",
    "                                 nonlinearity=lasagne.nonlinearities.linear)\n",
    "#l_out = lasagne.layers.RecurrentLayer(l_rec, n_out, \n",
    "#                                      W_in_to_hid=HeUniform(),\n",
    "#                                      W_hid_to_hid=HeUniform(),\n",
    "#                                      grad_clipping=10,\n",
    "#                                      nonlinearity=lasagne.nonlinearities.linear)\n",
    "model = LightweightModel([l_in], [l_out])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_variables = OrderedDict()\n",
    "input_variables[\"X\"] = dict(tensor_type=T.tensor3)\n",
    "input_variables[\"y\"] = dict(tensor_type=T.tensor3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from lasagnekit.easy import log_sum_exp\n",
    "\n",
    "def softmax(x, axis=1):\n",
    "    e_x = T.exp(x - x.max(axis=axis, keepdims=True))\n",
    "    out = e_x / e_x.sum(axis=axis, keepdims=True)\n",
    "    return out\n",
    "    \n",
    "class MyBatchOptimizer(BatchOptimizer):\n",
    "    \n",
    "    def iter_update(self, epoch, nb_batches, iter_update_batch):\n",
    "        status = super(MyBatchOptimizer, self).iter_update(epoch, nb_batches, iter_update_batch)\n",
    "        return status\n",
    "    \n",
    "batch_optimizer = MyBatchOptimizer(\n",
    "    verbose=1,\n",
    "    max_nb_epochs=1000,\n",
    "    batch_size=1,\n",
    "    optimization_procedure=(updates.rmsprop, \n",
    "                            {\"learning_rate\": 0.0001}),\n",
    ")\n",
    "\n",
    "def gaussian_ll(X, x_mean_hat, x_log_sigma_hat):\n",
    "    # x_mean_hat and x_sigma_hat computed by a neural network\n",
    "    x_hat = X - x_mean_hat\n",
    "    ll = -0.5 * ((x_hat ** 2) / T.exp(2. * x_log_sigma_hat) + 2 * x_log_sigma_hat) - 0.5*np.log(2*np.pi)\n",
    "    return ll\n",
    "\n",
    "def extract_from_y_dist(y_dist):\n",
    "    y_dist_mixtures =    y_dist[:, :, 0:size_mixture*8*2]\n",
    "    y_dist_mixtures = y_dist_mixtures.reshape((y_dist_mixtures.shape[0], \n",
    "                                               y_dist_mixtures.shape[1], \n",
    "                                               size_mixture, 2, 8))\n",
    "    y_mean = T.tanh(y_dist_mixtures[:, :, :, 0, :])\n",
    "    y_std = T.exp(y_dist_mixtures[:, :, :, 1, :])\n",
    "    \n",
    "    #100, 25, 7, 8\n",
    "    y_dist_proportions = y_dist[:, :, size_mixture*8*2:]\n",
    "    y_dist_proportions = y_dist_proportions.reshape((y_dist_proportions.shape[0],\n",
    "                                                     y_dist_proportions.shape[1],\n",
    "                                                     size_mixture,\n",
    "                                                    ))\n",
    "    # 100, 25, 7\n",
    "    y_dist_proportions = softmax(y_dist_proportions, axis=2)\n",
    "    return y_mean, y_std, y_dist_proportions\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def loss_function(model, tensors):\n",
    "    tx = tensors[\"X\"]\n",
    "    ty = tensors[\"y\"]\n",
    "    \n",
    "    x = tx\n",
    "    y_dist, = model.get_output(x)\n",
    "    #8 means, 8 stds for each mixture\n",
    "    \n",
    "    y_mean, y_std, y_dist_proportions = extract_from_y_dist(y_dist)\n",
    "\n",
    "    #y_mean : 100, 25, 7, 8\n",
    "    #y_std  : 100, 25, 7, 8\n",
    "    #dist_proportions : 100, 25, 7, 8\n",
    "    # y     : 100, 25, 8\n",
    "    \n",
    "    y_ = ty.dimshuffle(0, 1, 'x', 2)\n",
    "    \n",
    "    #100, 25, 7\n",
    "    per_mixture = gaussian_ll(y_, y_mean, y_std).sum(axis=3) + T.log(y_dist_proportions)\n",
    "    #100, 25, 7\n",
    "    per_example_and_time = log_sum_exp(per_mixture, axis=2)\n",
    "    per_example = per_example_and_time.sum(axis=1).mean()\n",
    "    return -per_example\n",
    "\n",
    "functions = dict(\n",
    "    encode=dict(\n",
    "        get_output=lambda model, X:model.get_output(X)[0],\n",
    "        params=[\"X\"]\n",
    "    ),\n",
    "    extract_from_y_dist=dict(\n",
    "        get_output=lambda model, X: extract_from_y_dist(model.get_output(X)[0]),\n",
    "        params=[\"X\"]\n",
    "    )\n",
    ")\n",
    "\n",
    "capsule = Capsule(\n",
    "    input_variables, \n",
    "    model,\n",
    "    loss_function,\n",
    "    functions=functions,\n",
    "    batch_optimizer=batch_optimizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "outputs_normal = [o[0:-1] for o in outputs]\n",
    "outputs_shifted = [o[1:] for o in outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "      0           0         197.1\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "      1           0       198.932\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "      2           0        197.59\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "      3           0       197.513\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "      4           0       197.515\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "      5           0       197.504\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "      6           0       197.147\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "      7           0       197.115\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "      8           0       197.118\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "      9           0        197.15\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "     10           0       197.133\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "     11           0       197.136\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "     12           0       197.077\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "     13           0       197.071\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "     14           0       197.087\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "     15           0       197.085\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "     16           0       197.109\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "     17           0       197.102\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "     18           0       197.134\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "     19           0       197.084\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "     20           0       197.107\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "     21           0       197.041\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "     22           0       197.071\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "     23           0       197.065\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "     24           0       197.148\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "     25           0       197.158\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "     26           0       197.168\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "     27           0       197.092\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "     28           0       197.039\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "     29           0       197.041\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "     30           0       197.066\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "     31           0       197.107\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "     32           0       197.121\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "     33           0       197.115\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "     34           0       197.082\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "     35           0       197.084\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "     36           0       197.076\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "     37           0       197.157\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "     38           0       197.085\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "     39           0       197.148\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "     40           0       197.334\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "     41           0       197.189\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "     42           0       197.048\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "     43           0       197.009\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "     44           0       197.006\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "     45           0       197.045\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "     46           0       197.046\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "     47           0       197.067\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "     48           0       197.064\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "     49           0       197.133\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "     50           0       196.987\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "     51           0       196.996\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "     52           0       197.017\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "     53           0        197.06\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "     54           0       197.082\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "     55           0       197.123\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "     56           0       197.109\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "     57           0       197.109\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "     58           0       197.018\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "     59           0       197.008\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "     60           0       197.036\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "     61           0       197.052\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "     62           0       197.088\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "     63           0       197.075\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "     64           0       197.044\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "     65           0       197.031\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "     66           0           197\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "     67           0       197.006\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "     68           0       197.016\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "     69           0       197.054\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "     70           0       197.112\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "     71           0        197.12\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "     72           0       197.174\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "     73           0       196.998\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "     74           0       196.971\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "     75           0       196.967\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "     76           0       196.993\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "     77           0       197.008\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "     78           0       197.019\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "     79           0       197.055\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "     80           0       196.986\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "     81           0       197.035\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "     82           0       197.122\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "     83           0       197.113\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "     84           0       197.059\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "     85           0       197.007\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "     86           0       196.977\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "     87           0       196.966\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "     88           0       196.967\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "     89           0        196.98\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "     90           0       197.041\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "     91           0       196.988\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "     92           0       197.076\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "     93           0       196.981\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "     94           0       197.019\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "     95           0       197.054\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "     96           0       197.087\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "     97           0       197.084\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "     98           0       197.226\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "     99           0       197.166\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    100           0       197.151\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    101           0       197.006\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    102           0       196.998\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    103           0       197.051\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    104           0       196.993\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    105           0       196.963\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    106           0       196.943\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    107           0       196.981\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    108           0       196.937\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    109           0       196.967\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    110           0       196.958\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    111           0       196.966\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    112           0       196.944\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    113           0       196.989\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    114           0       196.998\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    115           0       197.017\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    116           0       197.002\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    117           0       197.011\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    118           0       196.981\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    119           0       196.996\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    120           0       196.981\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    121           0       196.977\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    122           0       196.967\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    123           0       196.965\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    124           0        196.95\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    125           0       196.947\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    126           0       196.933\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    127           0        196.96\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    128           0       196.965\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    129           0       196.994\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    130           0       196.997\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    131           0       197.021\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    132           0       197.027\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    133           0       197.032\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    134           0       196.929\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    135           0       196.921\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    136           0       196.904\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    137           0        196.92\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    138           0       196.915\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    139           0       196.946\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    140           0       196.949\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    141           0       197.009\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    142           0        197.07\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    143           0       197.176\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    144           0       197.003\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    145           0       196.935\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    146           0       196.896\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    147           0       196.915\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    148           0       196.909\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    149           0       196.913\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    150           0       196.921\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    151           0       196.991\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    152           0       197.045\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    153           0       197.048\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    154           0       196.976\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    155           0       196.914\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    156           0       196.891\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    157           0       196.882\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    158           0       196.872\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    159           0        196.88\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    160           0       196.892\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    161           0       196.933\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    162           0       197.008\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    163           0       196.943\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    164           0       196.965\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    165           0       196.934\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    166           0       196.935\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    167           0       196.901\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    168           0       196.905\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    169           0       196.872\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    170           0       196.894\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    171           0       196.889\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    172           0       196.949\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    173           0       197.002\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    174           0        197.03\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    175           0       196.998\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    176           0       196.929\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    177           0       196.878\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    178           0       196.863\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    179           0       196.866\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    180           0       196.923\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    181           0       196.934\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    182           0       196.977\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    183           0       196.855\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    184           0       196.845\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    185           0       196.834\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    186           0       196.859\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    187           0       196.846\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    188           0       196.878\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    189           0       196.861\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    190           0       196.908\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    191           0       196.906\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    192           0       196.967\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    193           0        196.91\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    194           0       196.897\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    195           0       196.899\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    196           0       196.941\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    197           0       196.939\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    198           0       197.024\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    199           0        197.16\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    200           0       196.978\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    201           0       196.838\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    202           0       196.801\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    203           0        196.82\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    204           0       196.868\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    205           0       196.894\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    206           0       196.905\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    207           0       196.866\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    208           0       196.875\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    209           0       196.867\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    210           0       196.875\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    211           0       196.875\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    212           0       196.916\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    213           0       196.901\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    214           0       196.891\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    215           0       196.816\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    216           0       196.822\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    217           0       196.804\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    218           0       196.842\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    219           0       196.844\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    220           0       196.856\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    221           0       196.829\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    222           0       196.837\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    223           0       196.844\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    224           0       196.876\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    225           0       196.873\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    226           0       196.844\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    227           0       196.824\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    228           0       196.862\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    229           0       196.871\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    230           0       196.922\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    231           0       196.843\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    232           0       196.834\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    233           0       196.815\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    234           0        196.86\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    235           0       196.847\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    236           0       196.915\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    237           0       196.872\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    238           0       196.864\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    239           0       196.813\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    240           0       196.808\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    241           0       196.793\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    242           0        196.81\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    243           0       196.788\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    244           0       196.818\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    245           0       196.789\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    246           0       196.832\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    247           0       196.795\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    248           0       196.846\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    249           0       196.838\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    250           0       196.895\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    251           0       196.879\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    252           0       196.846\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    253           0        196.81\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    254           0        196.78\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    255           0       196.768\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    256           0       196.772\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    257           0       196.789\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    258           0       196.828\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    259           0       196.878\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    260           0       196.901\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    261           0       196.846\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    262           0       196.948\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    263           0       197.077\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    264           0       196.986\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    265           0       196.816\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    266           0       196.775\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    267           0        196.77\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    268           0       196.814\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    269           0       196.794\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    270           0       196.808\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    271           0       196.776\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    272           0       196.767\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    273           0       196.768\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    274           0       196.792\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    275           0       196.782\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    276           0       196.795\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    277           0        196.78\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    278           0       196.807\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    279           0       196.775\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    280           0       196.789\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    281           0        196.74\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    282           0       196.742\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    283           0       196.732\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    284           0       196.761\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    285           0       196.766\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    286           0       196.802\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    287           0       196.798\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    288           0       196.814\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    289           0       196.807\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    290           0       196.777\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    291           0       196.752\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    292           0       196.727\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    293           0       196.723\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    294           0       196.734\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    295           0       196.763\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    296           0        196.79\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    297           0       196.834\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    298           0       196.745\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    299           0        196.75\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    300           0       196.721\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    301           0       196.757\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    302           0       196.744\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    303           0       196.799\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    304           0        196.81\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    305           0       196.798\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    306           0       196.772\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    307           0       196.739\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    308           0       196.721\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    309           0       196.712\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    310           0       196.722\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    311           0       196.746\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    312           0       196.773\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    313           0       196.804\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    314           0       196.771\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    315           0       196.753\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    316           0       196.743\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    317           0       196.753\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    318           0       196.744\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    319           0       196.749\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    320           0       196.729\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    321           0       196.759\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    322           0       196.747\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    323           0       196.776\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    324           0       196.713\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    325           0       196.714\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    326           0       196.705\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    327           0       196.718\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    328           0       196.729\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    329           0       196.735\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    330           0       196.749\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    331           0       196.754\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    332           0       196.791\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    333           0       196.764\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    334           0       196.767\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    335           0       196.725\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    336           0       196.705\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    337           0       196.677\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    338           0       196.672\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    339           0       196.661\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    340           0       196.662\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    341           0       196.664\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    342           0       196.688\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    343           0       196.717\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    344           0        196.77\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    345           0       196.727\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    346           0       196.705\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    347           0       196.682\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    348           0       196.689\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    349           0       196.693\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    350           0        196.73\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    351           0       196.737\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    352           0       196.769\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    353           0       196.684\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    354           0       196.684\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    355           0       196.663\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    356           0       196.694\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    357           0       196.696\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    358           0       196.718\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    359           0       196.696\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    360           0       196.679\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    361           0       196.661\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    362           0       196.666\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    363           0       196.674\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    364           0       196.687\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    365           0       196.684\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    366           0       196.676\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    367           0       196.678\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    368           0       196.638\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    369           0       196.639\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    370           0       196.636\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    371           0        196.66\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    372           0       196.733\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    373           0       196.698\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    374           0       196.727\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    375           0       196.689\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    376           0       196.631\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    377           0        196.61\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    378           0       196.625\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    379           0       196.632\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    380           0       196.674\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    381           0       196.686\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    382           0        196.71\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    383           0        196.71\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    384           0       196.723\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    385           0       196.666\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    386           0        196.64\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    387           0       196.628\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    388           0       196.635\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    389           0       196.661\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    390           0       196.674\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    391           0       196.732\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    392           0       196.622\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    393           0       196.637\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    394           0       196.645\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    395           0       196.671\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    396           0       196.656\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    397           0       196.669\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    398           0       196.674\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    399           0        196.69\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    400           0        196.67\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    401           0       196.659\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    402           0       196.656\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    403           0       196.653\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    404           0       196.685\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    405           0       196.628\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    406           0       196.687\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    407           0       196.909\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    408           0       196.875\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    409           0       196.679\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    410           0       196.595\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    411           0       196.572\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    412           0       196.582\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    413           0       196.601\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    414           0       196.656\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    415           0       196.638\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    416           0       196.658\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    417           0       196.615\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    418           0       196.622\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    419           0       196.598\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    420           0       196.608\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    421           0       196.596\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    422           0       196.612\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    423           0       196.605\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    424           0       196.628\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    425           0       196.622\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    426           0       196.616\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    427           0       196.597\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    428           0       196.593\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    429           0       196.586\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    430           0       196.614\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    431           0       196.616\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    432           0       196.632\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    433           0       196.599\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    434           0       196.595\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    435           0       196.561\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    436           0        196.58\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    437           0       196.561\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    438           0       196.615\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    439           0       196.621\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    440           0       196.674\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    441           0       196.638\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    442           0       196.622\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    443           0       196.589\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    444           0       196.572\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    445           0       196.561\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    446           0       196.568\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    447           0       196.574\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    448           0       196.594\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    449           0       196.602\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    450           0        196.61\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    451           0       196.621\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    452           0       196.621\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    453           0       196.641\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    454           0       196.608\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    455           0       196.621\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    456           0       196.559\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    457           0       196.571\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    458           0       196.573\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    459           0       196.595\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    460           0       196.582\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    461           0       196.577\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    462           0       196.556\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    463           0       196.545\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    464           0       196.539\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    465           0       196.535\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    466           0       196.537\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    467           0       196.539\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    468           0       196.539\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    469           0       196.553\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    470           0       196.564\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    471           0       196.606\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    472           0       196.596\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    473           0       196.594\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    474           0       196.573\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    475           0       196.665\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    476           0       196.904\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    477           0       196.584\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    478           0       196.519\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    479           0       196.571\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    480           0        196.55\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    481           0       196.556\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    482           0       196.545\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    483           0        196.54\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    484           0       196.541\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    485           0       196.545\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    486           0       196.551\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    487           0        196.55\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    488           0        196.56\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    489           0       196.566\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    490           0       196.561\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    491           0       196.552\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    492           0        196.54\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    493           0       196.535\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    494           0       196.526\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    495           0       196.519\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    496           0       196.536\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    497           0       196.516\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    498           0       196.535\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    499           0       196.499\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    500           0       196.519\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    501           0       196.503\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    502           0       196.533\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    503           0       196.518\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    504           0       196.561\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    505           0       196.505\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    506           0       196.533\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    507           0       196.528\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    508           0       196.542\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    509           0       196.523\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    510           0       196.522\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    511           0        196.52\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    512           0        196.53\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    513           0       196.532\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    514           0        196.51\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    515           0       196.499\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    516           0       196.503\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    517           0        196.51\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    518           0       196.537\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    519           0       196.536\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    520           0        196.55\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    521           0       196.516\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    522           0       196.502\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    523           0       196.482\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    524           0       196.478\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    525           0        196.47\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    526           0       196.482\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    527           0       196.486\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    528           0       196.515\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    529           0       196.525\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    530           0       196.551\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    531           0       196.523\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    532           0       196.513\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    533           0       196.486\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    534           0       196.487\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    535           0       196.477\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    536           0        196.49\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    537           0       196.479\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    538           0        196.49\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    539           0       196.464\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    540           0       196.472\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    541           0       196.466\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    542           0       196.507\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    543           0       196.502\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    544           0       196.534\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    545           0       196.541\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    546           0       196.634\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    547           0       196.782\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    548           0       196.594\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    549           0        196.48\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    550           0       196.456\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    551           0       196.471\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    552           0       196.477\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    553           0       196.465\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    554           0       196.461\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    555           0       196.445\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    556           0       196.446\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    557           0       196.437\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    558           0       196.453\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    559           0       196.466\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    560           0        196.51\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    561           0       196.483\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    562           0       196.481\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    563           0       196.463\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    564           0         196.5\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    565           0       196.462\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    566           0       196.467\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    567           0       196.454\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    568           0       196.468\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    569           0       196.469\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    570           0       196.472\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    571           0       196.455\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    572           0        196.44\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    573           0       196.429\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    574           0       196.424\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    575           0       196.435\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    576           0       196.423\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    577           0       196.442\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    578           0       196.431\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    579           0       196.444\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    580           0       196.434\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    581           0       196.436\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    582           0       196.431\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    583           0       196.449\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    584           0        196.47\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    585           0        196.51\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    586           0       196.427\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    587           0       196.432\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    588           0       196.441\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    589           0       196.438\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    590           0       196.445\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    591           0       196.446\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    592           0       196.472\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    593           0       196.427\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    594           0       196.434\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    595           0        196.41\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    596           0       196.417\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    597           0       196.413\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    598           0       196.413\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    599           0       196.416\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    600           0       196.417\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    601           0       196.425\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    602           0       196.442\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    603           0        196.44\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    604           0        196.46\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    605           0       196.448\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    606           0       196.434\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    607           0       196.418\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    608           0       196.413\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    609           0       196.412\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    610           0       196.417\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    611           0       196.436\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    612           0       196.429\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    613           0       196.462\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    614           0        196.39\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    615           0       196.402\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    616           0       196.386\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    617           0       196.402\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    618           0       196.402\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    619           0       196.418\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    620           0        196.41\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    621           0       196.406\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    622           0       196.402\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    623           0       196.386\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    624           0        196.38\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    625           0       196.373\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    626           0       196.377\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    627           0       196.395\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    628           0       196.393\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    629           0       196.389\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    630           0       196.374\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    631           0        196.37\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    632           0        196.37\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    633           0       196.378\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    634           0       196.379\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    635           0       196.386\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    636           0       196.387\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    637           0       196.397\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    638           0       196.418\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    639           0       196.456\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    640           0       196.465\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    641           0       196.411\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    642           0       196.378\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    643           0        196.36\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    644           0       196.353\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    645           0       196.355\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    646           0       196.341\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    647           0        196.37\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    648           0       196.338\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    649           0       196.382\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    650           0       196.372\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    651           0       196.433\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    652           0        196.42\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    653           0       196.396\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    654           0       196.359\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    655           0        196.35\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    656           0       196.351\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    657           0       196.365\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    658           0       196.381\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    659           0       196.391\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    660           0       196.381\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    661           0       196.367\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    662           0       196.357\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    663           0       196.344\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    664           0       196.344\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    665           0       196.349\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    666           0       196.336\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    667           0       196.344\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    668           0        196.34\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    669           0       196.379\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    670           0        196.38\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    671           0       196.394\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    672           0       196.385\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    673           0        196.38\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    674           0       196.356\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    675           0       196.348\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    676           0        196.32\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    677           0       196.324\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    678           0       196.308\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    679           0       196.327\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    680           0       196.326\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    681           0       196.372\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    682           0       196.363\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    683           0       196.377\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    684           0       196.318\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    685           0       196.304\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    686           0       196.289\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    687           0       196.308\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    688           0       196.324\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    689           0        196.37\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    690           0       196.359\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    691           0       196.333\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    692           0       196.311\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    693           0       196.314\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    694           0       196.307\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    695           0       196.306\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    696           0       196.308\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    697           0       196.325\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    698           0       196.349\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    699           0       196.358\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    700           0       196.354\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    701           0       196.319\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    702           0       196.313\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    703           0       196.312\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    704           0       196.319\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    705           0       196.339\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    706           0       196.341\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    707           0       196.366\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    708           0       196.336\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    709           0       196.321\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    710           0         196.3\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    711           0       196.306\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    712           0       196.283\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    713           0       196.295\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    714           0       196.277\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    715           0       196.294\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    716           0       196.288\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    717           0       196.326\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    718           0       196.288\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    719           0       196.295\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    720           0       196.278\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    721           0       196.296\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    722           0       196.301\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    723           0       196.315\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    724           0       196.319\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    725           0       196.328\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    726           0       196.338\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    727           0        196.33\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    728           0       196.329\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    729           0       196.277\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    730           0       196.277\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    731           0        196.28\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    732           0       196.304\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    733           0       196.293\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    734           0       196.305\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    735           0       196.295\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    736           0       196.293\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    737           0       196.287\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    738           0       196.291\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    739           0       196.295\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    740           0       196.303\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    741           0       196.292\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    742           0        196.28\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    743           0       196.259\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    744           0       196.255\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    745           0       196.246\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    746           0       196.259\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    747           0       196.265\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    748           0       196.281\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    749           0       196.286\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    750           0       196.266\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    751           0       196.269\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    752           0       196.261\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    753           0       196.276\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    754           0       196.279\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    755           0       196.308\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    756           0       196.282\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    757           0       196.292\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    758           0       196.247\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    759           0       196.256\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    760           0       196.251\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    761           0       196.275\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    762           0       196.277\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    763           0       196.284\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    764           0       196.273\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    765           0       196.264\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    766           0       196.253\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    767           0       196.253\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    768           0       196.236\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    769           0       196.232\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    770           0       196.224\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    771           0       196.244\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    772           0       196.263\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    773           0       196.278\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    774           0       196.269\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    775           0       196.264\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    776           0       196.256\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    777           0       196.277\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    778           0       196.272\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    779           0       196.277\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    780           0       196.248\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    781           0       196.241\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    782           0       196.227\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    783           0       196.229\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    784           0       196.228\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    785           0       196.241\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    786           0       196.243\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    787           0       196.243\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    788           0       196.234\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    789           0       196.226\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    790           0       196.232\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    791           0       196.236\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    792           0       196.254\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    793           0       196.215\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    794           0       196.221\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    795           0       196.213\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    796           0       196.234\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    797           0       196.215\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    798           0       196.231\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    799           0       196.213\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    800           0       196.222\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    801           0       196.204\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    802           0       196.205\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    803           0       196.202\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    804           0       196.218\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    805           0       196.235\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    806           0       196.253\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    807           0       196.256\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    808           0       196.268\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    809           0       196.265\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    810           0       196.254\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    811           0        196.22\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    812           0       196.199\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    813           0       196.186\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    814           0       196.187\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    815           0       196.187\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    816           0       196.197\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    817           0       196.199\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    818           0       196.202\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    819           0       196.193\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    820           0       196.191\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    821           0       196.189\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    822           0       196.202\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    823           0       196.215\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    824           0       196.241\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    825           0       196.247\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    826           0       196.245\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    827           0       196.208\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    828           0       196.194\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    829           0       196.161\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    830           0       196.163\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    831           0       196.157\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    832           0       196.179\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    833           0       196.185\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    834           0       196.206\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    835           0         196.2\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    836           0       196.191\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    837           0       196.185\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    838           0       196.172\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    839           0       196.172\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    840           0        196.17\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    841           0       196.187\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    842           0       196.198\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    843           0       196.222\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    844           0       196.177\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    845           0       196.174\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    846           0       196.168\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    847           0       196.169\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    848           0       196.169\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    849           0       196.174\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    850           0       196.173\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    851           0       196.177\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    852           0       196.167\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    853           0       196.167\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    854           0        196.16\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    855           0       196.168\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    856           0       196.176\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    857           0       196.163\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    858           0       196.149\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    859           0       196.135\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    860           0       196.131\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    861           0        196.15\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    862           0       196.148\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    863           0       196.203\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    864           0        196.22\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    865           0        196.25\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    866           0       196.198\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    867           0       196.173\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    868           0       196.145\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    869           0       196.157\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    870           0       196.163\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    871           0       196.174\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    872           0       196.156\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    873           0       196.144\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    874           0       196.119\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    875           0        196.12\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    876           0       196.107\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    877           0       196.125\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    878           0       196.126\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    879           0       196.162\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    880           0       196.154\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    881           0       196.139\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    882           0       196.121\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    883           0       196.122\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    884           0       196.129\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    885           0       196.151\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    886           0       196.164\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    887           0       196.151\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    888           0       196.153\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    889           0       196.138\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    890           0       196.156\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    891           0       196.162\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    892           0       196.176\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    893           0       196.155\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    894           0       196.155\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    895           0       196.127\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    896           0       196.129\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    897           0       196.123\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    898           0       196.116\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    899           0        196.11\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    900           0        196.11\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    901           0       196.118\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    902           0        196.12\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    903           0        196.13\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    904           0        196.11\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    905           0       196.104\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    906           0         196.1\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    907           0       196.108\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    908           0       196.126\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    909           0       196.131\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    910           0       196.141\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    911           0       196.123\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    912           0       196.138\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    913           0       196.096\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    914           0       196.101\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    915           0        196.08\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    916           0       196.093\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    917           0       196.086\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    918           0       196.108\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    919           0       196.116\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    920           0       196.121\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    921           0       196.122\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    922           0       196.113\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    923           0       196.115\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    924           0       196.103\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    925           0       196.115\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    926           0       196.103\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    927           0       196.119\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    928           0       196.111\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    929           0       196.114\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    930           0         196.1\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    931           0       196.092\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    932           0       196.081\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    933           0       196.083\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    934           0       196.076\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    935           0       196.079\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    936           0       196.073\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    937           0       196.079\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    938           0       196.089\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    939           0       196.095\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    940           0        196.11\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    941           0       196.124\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    942           0       196.135\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    943           0         196.1\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    944           0        196.09\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    945           0       196.069\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    946           0       196.074\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    947           0       196.071\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    948           0       196.069\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    949           0       196.065\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    950           0       196.073\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    951           0       196.091\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    952           0       196.107\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    953           0       196.114\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    954           0       196.095\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    955           0       196.082\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    956           0       196.081\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    957           0       196.062\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    958           0       196.079\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    959           0       196.062\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    960           0        196.08\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    961           0       196.066\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    962           0       196.068\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    963           0        196.05\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    964           0       196.049\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    965           0       196.042\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    966           0       196.042\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    967           0       196.043\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    968           0       196.046\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    969           0        196.06\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    970           0       196.065\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    971           0       196.086\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    972           0       196.096\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    973           0       196.084\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    974           0       196.081\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    975           0       196.057\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    976           0       196.067\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    977           0       196.054\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    978           0       196.073\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    979           0       196.071\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    980           0       196.084\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    981           0       196.069\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    982           0       196.059\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    983           0       196.039\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    984           0       196.037\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    985           0       196.033\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    986           0       196.051\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    987           0       196.049\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    988           0       196.059\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    989           0       196.046\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    990           0       196.042\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    991           0       196.029\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    992           0       196.036\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    993           0       196.032\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    994           0        196.05\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    995           0       196.037\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    996           0       196.038\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    997           0       196.018\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    998           0       196.024\n",
      "  epoch    loss_std    loss_train\n",
      "-------  ----------  ------------\n",
      "    999           0       196.021\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<lasagnekit.generative.capsule.Capsule at 0x7f554f360090>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "capsule.fit(X=outputs_normal[0:1], y=outputs_shifted[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f54c953db10>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAEACAYAAABbMHZzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nM7MC+v/xAsdpD1sydAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f54b42f0110>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from lasagnekit.easy import get_stat\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(get_stat(\"loss_train\", capsule.batch_optimizer.stats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#np.abs(capsule.encode(inputs[0:1])-outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "<svg version=\"1.0\" width=\"300\" height=\"300\"\n",
    "     xmlns=\"http://www.w3.org/2000/svg\" \n",
    "     xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
    "     {0}\n",
    "</svg> \n",
    "\"\"\"\n",
    "path_template = \"\"\"\"<path d=\"M {x1},{y1} C {cx},{cy} {x2},{y2} {x3},{y3}\" \n",
    "                          stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\"\"\"\n",
    "\n",
    "def gen_path(x1, y1, cx, cy, x2, y2, x3, y3):\n",
    "    return path_template.format(x1=x1, y1=y1, cx=cx, cy=cy, x2=x2, y2=y2, x3=x3, y3=y3)\n",
    "def gen_svg_from_output(output):\n",
    "    o = output.copy()\n",
    "    o[:, (0, 2, 4, 6)] *= 200\n",
    "    o[:, (0, 2, 4, 6)] += 50\n",
    "    o[:, (1, 3, 5, 7)] *= 200\n",
    "    o[:, (1, 3, 5, 7)] += 50\n",
    "    assert output.shape[1] == 8\n",
    "    s = \"\"\n",
    "    for out in o:\n",
    "        s += gen_path(*out) + \"\\n\"\n",
    "    return template.format(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate(model, initial=None, nb_steps=1):\n",
    "    if initial is None:\n",
    "        initial = np.zeros((8,)).astype(np.float32)\n",
    "    generated = np.zeros((1, nb_steps + 1, 8)).astype(np.float32)\n",
    "    generated[0, 0] = initial\n",
    "    # (one example, one time step, 8)\n",
    "    x = initial[None, None, :]\n",
    "    for i in range(1, nb_steps):\n",
    "        y_mean, y_std, y_dist_proportions = model.extract_from_y_dist(generated[:, 0:i, :])\n",
    "        y_dist_proportions = y_dist_proportions[0, i - 1] #** 0.1\n",
    "        \n",
    "        mixture_id = y_dist_proportions.argmax()\n",
    "        y_dist_proportions[-1] = 1 - y_dist_proportions[0:-1].sum()\n",
    "        #mixture_id = np.random.multinomial(1, y_dist_proportions).argmax()\n",
    "        y_mean = y_mean[0, i - 1, mixture_id]\n",
    "        y_std = y_std[0, i - 1, mixture_id]\n",
    "        y_gen = np.random.multivariate_normal(y_mean, (np.diag(y_std**2)))\n",
    "     \n",
    "        #generated[i] = outputs[0][i]\n",
    "        generated[0, i, :] = y_gen# + generated[0, i - 1, :]\n",
    "        \n",
    "    return generated[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"300\" version=\"1.0\" width=\"300\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "     &quot;<path d=\"M 50.0,50.0 C 50.0,50.0 50.0,50.0 50.0,50.0\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "\n",
       "</svg>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"300\" version=\"1.0\" width=\"300\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "     &quot;<path d=\"M 50.0,50.0 C 50.0,50.0 50.0,50.0 50.0,50.0\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 191.115020752,75.9389953613 C 218.564575195,103.72063446 148.526672363,15.0162353516 82.3462295532,108.744041443\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "\n",
       "</svg>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"300\" version=\"1.0\" width=\"300\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "     &quot;<path d=\"M 50.0,50.0 C 50.0,50.0 50.0,50.0 50.0,50.0\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 191.115020752,75.9389953613 C 218.564575195,103.72063446 148.526672363,15.0162353516 82.3462295532,108.744041443\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 152.760375977,56.6826820374 C 149.854431152,62.57654953 156.112579346,98.9979171753 104.421005249,97.5242767334\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "\n",
       "</svg>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"300\" version=\"1.0\" width=\"300\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "     &quot;<path d=\"M 50.0,50.0 C 50.0,50.0 50.0,50.0 50.0,50.0\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 191.115020752,75.9389953613 C 218.564575195,103.72063446 148.526672363,15.0162353516 82.3462295532,108.744041443\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 152.760375977,56.6826820374 C 149.854431152,62.57654953 156.112579346,98.9979171753 104.421005249,97.5242767334\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 108.233200073,101.882278442 C 136.539230347,113.20514679 144.699661255,116.156349182 172.907104492,157.495025635\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "\n",
       "</svg>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"300\" version=\"1.0\" width=\"300\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "     &quot;<path d=\"M 50.0,50.0 C 50.0,50.0 50.0,50.0 50.0,50.0\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 191.115020752,75.9389953613 C 218.564575195,103.72063446 148.526672363,15.0162353516 82.3462295532,108.744041443\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 152.760375977,56.6826820374 C 149.854431152,62.57654953 156.112579346,98.9979171753 104.421005249,97.5242767334\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 108.233200073,101.882278442 C 136.539230347,113.20514679 144.699661255,116.156349182 172.907104492,157.495025635\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 164.241394043,138.595336914 C 160.649810791,117.082008362 168.714874268,114.288925171 176.743988037,104.940284729\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "\n",
       "</svg>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"300\" version=\"1.0\" width=\"300\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "     &quot;<path d=\"M 50.0,50.0 C 50.0,50.0 50.0,50.0 50.0,50.0\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 191.115020752,75.9389953613 C 218.564575195,103.72063446 148.526672363,15.0162353516 82.3462295532,108.744041443\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 152.760375977,56.6826820374 C 149.854431152,62.57654953 156.112579346,98.9979171753 104.421005249,97.5242767334\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 108.233200073,101.882278442 C 136.539230347,113.20514679 144.699661255,116.156349182 172.907104492,157.495025635\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 164.241394043,138.595336914 C 160.649810791,117.082008362 168.714874268,114.288925171 176.743988037,104.940284729\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 159.371765137,95.7409439087 C 195.13722229,100.245635986 224.69380188,134.492462158 230.739959717,169.187133789\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "\n",
       "</svg>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"300\" version=\"1.0\" width=\"300\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "     &quot;<path d=\"M 50.0,50.0 C 50.0,50.0 50.0,50.0 50.0,50.0\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 191.115020752,75.9389953613 C 218.564575195,103.72063446 148.526672363,15.0162353516 82.3462295532,108.744041443\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 152.760375977,56.6826820374 C 149.854431152,62.57654953 156.112579346,98.9979171753 104.421005249,97.5242767334\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 108.233200073,101.882278442 C 136.539230347,113.20514679 144.699661255,116.156349182 172.907104492,157.495025635\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 164.241394043,138.595336914 C 160.649810791,117.082008362 168.714874268,114.288925171 176.743988037,104.940284729\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 159.371765137,95.7409439087 C 195.13722229,100.245635986 224.69380188,134.492462158 230.739959717,169.187133789\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 221.917114258,153.861633301 C 195.628982544,195.038497925 210.012786865,216.327926636 183.361236572,230.869476318\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "\n",
       "</svg>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"300\" version=\"1.0\" width=\"300\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "     &quot;<path d=\"M 50.0,50.0 C 50.0,50.0 50.0,50.0 50.0,50.0\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 191.115020752,75.9389953613 C 218.564575195,103.72063446 148.526672363,15.0162353516 82.3462295532,108.744041443\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 152.760375977,56.6826820374 C 149.854431152,62.57654953 156.112579346,98.9979171753 104.421005249,97.5242767334\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 108.233200073,101.882278442 C 136.539230347,113.20514679 144.699661255,116.156349182 172.907104492,157.495025635\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 164.241394043,138.595336914 C 160.649810791,117.082008362 168.714874268,114.288925171 176.743988037,104.940284729\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 159.371765137,95.7409439087 C 195.13722229,100.245635986 224.69380188,134.492462158 230.739959717,169.187133789\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 221.917114258,153.861633301 C 195.628982544,195.038497925 210.012786865,216.327926636 183.361236572,230.869476318\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 161.959472656,224.108673096 C 195.834121704,237.330657959 174.061203003,240.171508789 177.696594238,239.656158447\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "\n",
       "</svg>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"300\" version=\"1.0\" width=\"300\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "     &quot;<path d=\"M 50.0,50.0 C 50.0,50.0 50.0,50.0 50.0,50.0\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 191.115020752,75.9389953613 C 218.564575195,103.72063446 148.526672363,15.0162353516 82.3462295532,108.744041443\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 152.760375977,56.6826820374 C 149.854431152,62.57654953 156.112579346,98.9979171753 104.421005249,97.5242767334\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 108.233200073,101.882278442 C 136.539230347,113.20514679 144.699661255,116.156349182 172.907104492,157.495025635\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 164.241394043,138.595336914 C 160.649810791,117.082008362 168.714874268,114.288925171 176.743988037,104.940284729\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 159.371765137,95.7409439087 C 195.13722229,100.245635986 224.69380188,134.492462158 230.739959717,169.187133789\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 221.917114258,153.861633301 C 195.628982544,195.038497925 210.012786865,216.327926636 183.361236572,230.869476318\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 161.959472656,224.108673096 C 195.834121704,237.330657959 174.061203003,240.171508789 177.696594238,239.656158447\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 156.195404053,248.846221924 C 208.15020752,239.646179199 248.822616577,208.73097229 239.494369507,175.876586914\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "\n",
       "</svg>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"300\" version=\"1.0\" width=\"300\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "     &quot;<path d=\"M 50.0,50.0 C 50.0,50.0 50.0,50.0 50.0,50.0\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 191.115020752,75.9389953613 C 218.564575195,103.72063446 148.526672363,15.0162353516 82.3462295532,108.744041443\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 152.760375977,56.6826820374 C 149.854431152,62.57654953 156.112579346,98.9979171753 104.421005249,97.5242767334\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 108.233200073,101.882278442 C 136.539230347,113.20514679 144.699661255,116.156349182 172.907104492,157.495025635\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 164.241394043,138.595336914 C 160.649810791,117.082008362 168.714874268,114.288925171 176.743988037,104.940284729\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 159.371765137,95.7409439087 C 195.13722229,100.245635986 224.69380188,134.492462158 230.739959717,169.187133789\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 221.917114258,153.861633301 C 195.628982544,195.038497925 210.012786865,216.327926636 183.361236572,230.869476318\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 161.959472656,224.108673096 C 195.834121704,237.330657959 174.061203003,240.171508789 177.696594238,239.656158447\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 156.195404053,248.846221924 C 208.15020752,239.646179199 248.822616577,208.73097229 239.494369507,175.876586914\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 243.070129395,166.774353027 C 242.012786865,116.916313171 219.480209351,92.0129394531 177.607208252,84.2614974976\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "\n",
       "</svg>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"300\" version=\"1.0\" width=\"300\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "     &quot;<path d=\"M 50.0,50.0 C 50.0,50.0 50.0,50.0 50.0,50.0\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 191.115020752,75.9389953613 C 218.564575195,103.72063446 148.526672363,15.0162353516 82.3462295532,108.744041443\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 152.760375977,56.6826820374 C 149.854431152,62.57654953 156.112579346,98.9979171753 104.421005249,97.5242767334\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 108.233200073,101.882278442 C 136.539230347,113.20514679 144.699661255,116.156349182 172.907104492,157.495025635\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 164.241394043,138.595336914 C 160.649810791,117.082008362 168.714874268,114.288925171 176.743988037,104.940284729\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 159.371765137,95.7409439087 C 195.13722229,100.245635986 224.69380188,134.492462158 230.739959717,169.187133789\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 221.917114258,153.861633301 C 195.628982544,195.038497925 210.012786865,216.327926636 183.361236572,230.869476318\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 161.959472656,224.108673096 C 195.834121704,237.330657959 174.061203003,240.171508789 177.696594238,239.656158447\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 156.195404053,248.846221924 C 208.15020752,239.646179199 248.822616577,208.73097229 239.494369507,175.876586914\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 243.070129395,166.774353027 C 242.012786865,116.916313171 219.480209351,92.0129394531 177.607208252,84.2614974976\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 161.544433594,84.4290237427 C 156.439956665,77.1265869141 166.847137451,90.7735443115 173.840377808,81.9592895508\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "\n",
       "</svg>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"300\" version=\"1.0\" width=\"300\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "     &quot;<path d=\"M 50.0,50.0 C 50.0,50.0 50.0,50.0 50.0,50.0\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 191.115020752,75.9389953613 C 218.564575195,103.72063446 148.526672363,15.0162353516 82.3462295532,108.744041443\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 152.760375977,56.6826820374 C 149.854431152,62.57654953 156.112579346,98.9979171753 104.421005249,97.5242767334\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 108.233200073,101.882278442 C 136.539230347,113.20514679 144.699661255,116.156349182 172.907104492,157.495025635\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 164.241394043,138.595336914 C 160.649810791,117.082008362 168.714874268,114.288925171 176.743988037,104.940284729\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 159.371765137,95.7409439087 C 195.13722229,100.245635986 224.69380188,134.492462158 230.739959717,169.187133789\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 221.917114258,153.861633301 C 195.628982544,195.038497925 210.012786865,216.327926636 183.361236572,230.869476318\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 161.959472656,224.108673096 C 195.834121704,237.330657959 174.061203003,240.171508789 177.696594238,239.656158447\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 156.195404053,248.846221924 C 208.15020752,239.646179199 248.822616577,208.73097229 239.494369507,175.876586914\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 243.070129395,166.774353027 C 242.012786865,116.916313171 219.480209351,92.0129394531 177.607208252,84.2614974976\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 161.544433594,84.4290237427 C 156.439956665,77.1265869141 166.847137451,90.7735443115 173.840377808,81.9592895508\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 84.1051635742,214.566650391 C 91.2473602295,235.932357788 121.864402771,216.515518188 144.375793457,239.611618042\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "\n",
       "</svg>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"300\" version=\"1.0\" width=\"300\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "     &quot;<path d=\"M 50.0,50.0 C 50.0,50.0 50.0,50.0 50.0,50.0\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 191.115020752,75.9389953613 C 218.564575195,103.72063446 148.526672363,15.0162353516 82.3462295532,108.744041443\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 152.760375977,56.6826820374 C 149.854431152,62.57654953 156.112579346,98.9979171753 104.421005249,97.5242767334\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 108.233200073,101.882278442 C 136.539230347,113.20514679 144.699661255,116.156349182 172.907104492,157.495025635\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 164.241394043,138.595336914 C 160.649810791,117.082008362 168.714874268,114.288925171 176.743988037,104.940284729\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 159.371765137,95.7409439087 C 195.13722229,100.245635986 224.69380188,134.492462158 230.739959717,169.187133789\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 221.917114258,153.861633301 C 195.628982544,195.038497925 210.012786865,216.327926636 183.361236572,230.869476318\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 161.959472656,224.108673096 C 195.834121704,237.330657959 174.061203003,240.171508789 177.696594238,239.656158447\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 156.195404053,248.846221924 C 208.15020752,239.646179199 248.822616577,208.73097229 239.494369507,175.876586914\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 243.070129395,166.774353027 C 242.012786865,116.916313171 219.480209351,92.0129394531 177.607208252,84.2614974976\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 161.544433594,84.4290237427 C 156.439956665,77.1265869141 166.847137451,90.7735443115 173.840377808,81.9592895508\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 84.1051635742,214.566650391 C 91.2473602295,235.932357788 121.864402771,216.515518188 144.375793457,239.611618042\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 130.280670166,244.402389526 C 123.296318054,236.498519897 133.595947266,241.165130615 146.818054199,235.137191772\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "\n",
       "</svg>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"300\" version=\"1.0\" width=\"300\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "     &quot;<path d=\"M 50.0,50.0 C 50.0,50.0 50.0,50.0 50.0,50.0\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 191.115020752,75.9389953613 C 218.564575195,103.72063446 148.526672363,15.0162353516 82.3462295532,108.744041443\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 152.760375977,56.6826820374 C 149.854431152,62.57654953 156.112579346,98.9979171753 104.421005249,97.5242767334\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 108.233200073,101.882278442 C 136.539230347,113.20514679 144.699661255,116.156349182 172.907104492,157.495025635\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 164.241394043,138.595336914 C 160.649810791,117.082008362 168.714874268,114.288925171 176.743988037,104.940284729\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 159.371765137,95.7409439087 C 195.13722229,100.245635986 224.69380188,134.492462158 230.739959717,169.187133789\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 221.917114258,153.861633301 C 195.628982544,195.038497925 210.012786865,216.327926636 183.361236572,230.869476318\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 161.959472656,224.108673096 C 195.834121704,237.330657959 174.061203003,240.171508789 177.696594238,239.656158447\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 156.195404053,248.846221924 C 208.15020752,239.646179199 248.822616577,208.73097229 239.494369507,175.876586914\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 243.070129395,166.774353027 C 242.012786865,116.916313171 219.480209351,92.0129394531 177.607208252,84.2614974976\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 161.544433594,84.4290237427 C 156.439956665,77.1265869141 166.847137451,90.7735443115 173.840377808,81.9592895508\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 84.1051635742,214.566650391 C 91.2473602295,235.932357788 121.864402771,216.515518188 144.375793457,239.611618042\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 130.280670166,244.402389526 C 123.296318054,236.498519897 133.595947266,241.165130615 146.818054199,235.137191772\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 136.500411987,231.131774902 C 114.200737,224.329299927 108.918144226,218.812179565 105.396697998,221.153717041\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "\n",
       "</svg>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"300\" version=\"1.0\" width=\"300\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "     &quot;<path d=\"M 50.0,50.0 C 50.0,50.0 50.0,50.0 50.0,50.0\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 191.115020752,75.9389953613 C 218.564575195,103.72063446 148.526672363,15.0162353516 82.3462295532,108.744041443\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 152.760375977,56.6826820374 C 149.854431152,62.57654953 156.112579346,98.9979171753 104.421005249,97.5242767334\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 108.233200073,101.882278442 C 136.539230347,113.20514679 144.699661255,116.156349182 172.907104492,157.495025635\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 164.241394043,138.595336914 C 160.649810791,117.082008362 168.714874268,114.288925171 176.743988037,104.940284729\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 159.371765137,95.7409439087 C 195.13722229,100.245635986 224.69380188,134.492462158 230.739959717,169.187133789\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 221.917114258,153.861633301 C 195.628982544,195.038497925 210.012786865,216.327926636 183.361236572,230.869476318\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 161.959472656,224.108673096 C 195.834121704,237.330657959 174.061203003,240.171508789 177.696594238,239.656158447\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 156.195404053,248.846221924 C 208.15020752,239.646179199 248.822616577,208.73097229 239.494369507,175.876586914\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 243.070129395,166.774353027 C 242.012786865,116.916313171 219.480209351,92.0129394531 177.607208252,84.2614974976\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 161.544433594,84.4290237427 C 156.439956665,77.1265869141 166.847137451,90.7735443115 173.840377808,81.9592895508\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 84.1051635742,214.566650391 C 91.2473602295,235.932357788 121.864402771,216.515518188 144.375793457,239.611618042\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 130.280670166,244.402389526 C 123.296318054,236.498519897 133.595947266,241.165130615 146.818054199,235.137191772\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 136.500411987,231.131774902 C 114.200737,224.329299927 108.918144226,218.812179565 105.396697998,221.153717041\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 99.8319854736,220.086364746 C 87.6996612549,211.598327637 96.3422164917,229.408737183 90.8463134766,232.295578003\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "\n",
       "</svg>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"300\" version=\"1.0\" width=\"300\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "     &quot;<path d=\"M 50.0,50.0 C 50.0,50.0 50.0,50.0 50.0,50.0\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 191.115020752,75.9389953613 C 218.564575195,103.72063446 148.526672363,15.0162353516 82.3462295532,108.744041443\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 152.760375977,56.6826820374 C 149.854431152,62.57654953 156.112579346,98.9979171753 104.421005249,97.5242767334\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 108.233200073,101.882278442 C 136.539230347,113.20514679 144.699661255,116.156349182 172.907104492,157.495025635\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 164.241394043,138.595336914 C 160.649810791,117.082008362 168.714874268,114.288925171 176.743988037,104.940284729\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 159.371765137,95.7409439087 C 195.13722229,100.245635986 224.69380188,134.492462158 230.739959717,169.187133789\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 221.917114258,153.861633301 C 195.628982544,195.038497925 210.012786865,216.327926636 183.361236572,230.869476318\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 161.959472656,224.108673096 C 195.834121704,237.330657959 174.061203003,240.171508789 177.696594238,239.656158447\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 156.195404053,248.846221924 C 208.15020752,239.646179199 248.822616577,208.73097229 239.494369507,175.876586914\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 243.070129395,166.774353027 C 242.012786865,116.916313171 219.480209351,92.0129394531 177.607208252,84.2614974976\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 161.544433594,84.4290237427 C 156.439956665,77.1265869141 166.847137451,90.7735443115 173.840377808,81.9592895508\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 84.1051635742,214.566650391 C 91.2473602295,235.932357788 121.864402771,216.515518188 144.375793457,239.611618042\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 130.280670166,244.402389526 C 123.296318054,236.498519897 133.595947266,241.165130615 146.818054199,235.137191772\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 136.500411987,231.131774902 C 114.200737,224.329299927 108.918144226,218.812179565 105.396697998,221.153717041\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 99.8319854736,220.086364746 C 87.6996612549,211.598327637 96.3422164917,229.408737183 90.8463134766,232.295578003\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 86.2730712891,229.86756897 C 78.6575927734,225.559783936 84.9899978638,230.537658691 90.6410369873,231.467681885\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "\n",
       "</svg>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"300\" version=\"1.0\" width=\"300\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "     &quot;<path d=\"M 50.0,50.0 C 50.0,50.0 50.0,50.0 50.0,50.0\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 191.115020752,75.9389953613 C 218.564575195,103.72063446 148.526672363,15.0162353516 82.3462295532,108.744041443\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 152.760375977,56.6826820374 C 149.854431152,62.57654953 156.112579346,98.9979171753 104.421005249,97.5242767334\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 108.233200073,101.882278442 C 136.539230347,113.20514679 144.699661255,116.156349182 172.907104492,157.495025635\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 164.241394043,138.595336914 C 160.649810791,117.082008362 168.714874268,114.288925171 176.743988037,104.940284729\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 159.371765137,95.7409439087 C 195.13722229,100.245635986 224.69380188,134.492462158 230.739959717,169.187133789\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 221.917114258,153.861633301 C 195.628982544,195.038497925 210.012786865,216.327926636 183.361236572,230.869476318\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 161.959472656,224.108673096 C 195.834121704,237.330657959 174.061203003,240.171508789 177.696594238,239.656158447\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 156.195404053,248.846221924 C 208.15020752,239.646179199 248.822616577,208.73097229 239.494369507,175.876586914\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 243.070129395,166.774353027 C 242.012786865,116.916313171 219.480209351,92.0129394531 177.607208252,84.2614974976\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 161.544433594,84.4290237427 C 156.439956665,77.1265869141 166.847137451,90.7735443115 173.840377808,81.9592895508\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 84.1051635742,214.566650391 C 91.2473602295,235.932357788 121.864402771,216.515518188 144.375793457,239.611618042\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 130.280670166,244.402389526 C 123.296318054,236.498519897 133.595947266,241.165130615 146.818054199,235.137191772\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 136.500411987,231.131774902 C 114.200737,224.329299927 108.918144226,218.812179565 105.396697998,221.153717041\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 99.8319854736,220.086364746 C 87.6996612549,211.598327637 96.3422164917,229.408737183 90.8463134766,232.295578003\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 86.2730712891,229.86756897 C 78.6575927734,225.559783936 84.9899978638,230.537658691 90.6410369873,231.467681885\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 74.7118682861,181.558639526 C 58.2888946533,163.444000244 60.0914344788,175.059753418 53.7447624207,184.605712891\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "\n",
       "</svg>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"300\" version=\"1.0\" width=\"300\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "     &quot;<path d=\"M 50.0,50.0 C 50.0,50.0 50.0,50.0 50.0,50.0\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 191.115020752,75.9389953613 C 218.564575195,103.72063446 148.526672363,15.0162353516 82.3462295532,108.744041443\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 152.760375977,56.6826820374 C 149.854431152,62.57654953 156.112579346,98.9979171753 104.421005249,97.5242767334\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 108.233200073,101.882278442 C 136.539230347,113.20514679 144.699661255,116.156349182 172.907104492,157.495025635\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 164.241394043,138.595336914 C 160.649810791,117.082008362 168.714874268,114.288925171 176.743988037,104.940284729\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 159.371765137,95.7409439087 C 195.13722229,100.245635986 224.69380188,134.492462158 230.739959717,169.187133789\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 221.917114258,153.861633301 C 195.628982544,195.038497925 210.012786865,216.327926636 183.361236572,230.869476318\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 161.959472656,224.108673096 C 195.834121704,237.330657959 174.061203003,240.171508789 177.696594238,239.656158447\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 156.195404053,248.846221924 C 208.15020752,239.646179199 248.822616577,208.73097229 239.494369507,175.876586914\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 243.070129395,166.774353027 C 242.012786865,116.916313171 219.480209351,92.0129394531 177.607208252,84.2614974976\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 161.544433594,84.4290237427 C 156.439956665,77.1265869141 166.847137451,90.7735443115 173.840377808,81.9592895508\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 84.1051635742,214.566650391 C 91.2473602295,235.932357788 121.864402771,216.515518188 144.375793457,239.611618042\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 130.280670166,244.402389526 C 123.296318054,236.498519897 133.595947266,241.165130615 146.818054199,235.137191772\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 136.500411987,231.131774902 C 114.200737,224.329299927 108.918144226,218.812179565 105.396697998,221.153717041\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 99.8319854736,220.086364746 C 87.6996612549,211.598327637 96.3422164917,229.408737183 90.8463134766,232.295578003\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 86.2730712891,229.86756897 C 78.6575927734,225.559783936 84.9899978638,230.537658691 90.6410369873,231.467681885\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 74.7118682861,181.558639526 C 58.2888946533,163.444000244 60.0914344788,175.059753418 53.7447624207,184.605712891\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 52.3242874146,180.995574951 C 51.9291381836,171.053207397 61.2986526489,197.552856445 75.2585067749,222.660263062\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "\n",
       "</svg>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"300\" version=\"1.0\" width=\"300\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "     &quot;<path d=\"M 50.0,50.0 C 50.0,50.0 50.0,50.0 50.0,50.0\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 191.115020752,75.9389953613 C 218.564575195,103.72063446 148.526672363,15.0162353516 82.3462295532,108.744041443\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 152.760375977,56.6826820374 C 149.854431152,62.57654953 156.112579346,98.9979171753 104.421005249,97.5242767334\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 108.233200073,101.882278442 C 136.539230347,113.20514679 144.699661255,116.156349182 172.907104492,157.495025635\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 164.241394043,138.595336914 C 160.649810791,117.082008362 168.714874268,114.288925171 176.743988037,104.940284729\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 159.371765137,95.7409439087 C 195.13722229,100.245635986 224.69380188,134.492462158 230.739959717,169.187133789\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 221.917114258,153.861633301 C 195.628982544,195.038497925 210.012786865,216.327926636 183.361236572,230.869476318\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 161.959472656,224.108673096 C 195.834121704,237.330657959 174.061203003,240.171508789 177.696594238,239.656158447\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 156.195404053,248.846221924 C 208.15020752,239.646179199 248.822616577,208.73097229 239.494369507,175.876586914\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 243.070129395,166.774353027 C 242.012786865,116.916313171 219.480209351,92.0129394531 177.607208252,84.2614974976\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 161.544433594,84.4290237427 C 156.439956665,77.1265869141 166.847137451,90.7735443115 173.840377808,81.9592895508\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 84.1051635742,214.566650391 C 91.2473602295,235.932357788 121.864402771,216.515518188 144.375793457,239.611618042\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 130.280670166,244.402389526 C 123.296318054,236.498519897 133.595947266,241.165130615 146.818054199,235.137191772\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 136.500411987,231.131774902 C 114.200737,224.329299927 108.918144226,218.812179565 105.396697998,221.153717041\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 99.8319854736,220.086364746 C 87.6996612549,211.598327637 96.3422164917,229.408737183 90.8463134766,232.295578003\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 86.2730712891,229.86756897 C 78.6575927734,225.559783936 84.9899978638,230.537658691 90.6410369873,231.467681885\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 74.7118682861,181.558639526 C 58.2888946533,163.444000244 60.0914344788,175.059753418 53.7447624207,184.605712891\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 52.3242874146,180.995574951 C 51.9291381836,171.053207397 61.2986526489,197.552856445 75.2585067749,222.660263062\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 73.4063568115,220.320556641 C 67.2691802979,195.597991943 80.3762130737,202.118179321 89.1727981567,214.480789185\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "\n",
       "</svg>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"300\" version=\"1.0\" width=\"300\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "     &quot;<path d=\"M 50.0,50.0 C 50.0,50.0 50.0,50.0 50.0,50.0\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 191.115020752,75.9389953613 C 218.564575195,103.72063446 148.526672363,15.0162353516 82.3462295532,108.744041443\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 152.760375977,56.6826820374 C 149.854431152,62.57654953 156.112579346,98.9979171753 104.421005249,97.5242767334\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 108.233200073,101.882278442 C 136.539230347,113.20514679 144.699661255,116.156349182 172.907104492,157.495025635\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 164.241394043,138.595336914 C 160.649810791,117.082008362 168.714874268,114.288925171 176.743988037,104.940284729\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 159.371765137,95.7409439087 C 195.13722229,100.245635986 224.69380188,134.492462158 230.739959717,169.187133789\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 221.917114258,153.861633301 C 195.628982544,195.038497925 210.012786865,216.327926636 183.361236572,230.869476318\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 161.959472656,224.108673096 C 195.834121704,237.330657959 174.061203003,240.171508789 177.696594238,239.656158447\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 156.195404053,248.846221924 C 208.15020752,239.646179199 248.822616577,208.73097229 239.494369507,175.876586914\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 243.070129395,166.774353027 C 242.012786865,116.916313171 219.480209351,92.0129394531 177.607208252,84.2614974976\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 161.544433594,84.4290237427 C 156.439956665,77.1265869141 166.847137451,90.7735443115 173.840377808,81.9592895508\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 84.1051635742,214.566650391 C 91.2473602295,235.932357788 121.864402771,216.515518188 144.375793457,239.611618042\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 130.280670166,244.402389526 C 123.296318054,236.498519897 133.595947266,241.165130615 146.818054199,235.137191772\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 136.500411987,231.131774902 C 114.200737,224.329299927 108.918144226,218.812179565 105.396697998,221.153717041\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 99.8319854736,220.086364746 C 87.6996612549,211.598327637 96.3422164917,229.408737183 90.8463134766,232.295578003\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 86.2730712891,229.86756897 C 78.6575927734,225.559783936 84.9899978638,230.537658691 90.6410369873,231.467681885\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 74.7118682861,181.558639526 C 58.2888946533,163.444000244 60.0914344788,175.059753418 53.7447624207,184.605712891\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 52.3242874146,180.995574951 C 51.9291381836,171.053207397 61.2986526489,197.552856445 75.2585067749,222.660263062\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 73.4063568115,220.320556641 C 67.2691802979,195.597991943 80.3762130737,202.118179321 89.1727981567,214.480789185\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 85.4886550903,211.259338379 C 78.0659103394,181.797073364 72.9622039795,187.897644043 75.1873245239,184.543304443\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "\n",
       "</svg>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"300\" version=\"1.0\" width=\"300\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "     &quot;<path d=\"M 50.0,50.0 C 50.0,50.0 50.0,50.0 50.0,50.0\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 191.115020752,75.9389953613 C 218.564575195,103.72063446 148.526672363,15.0162353516 82.3462295532,108.744041443\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 152.760375977,56.6826820374 C 149.854431152,62.57654953 156.112579346,98.9979171753 104.421005249,97.5242767334\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 108.233200073,101.882278442 C 136.539230347,113.20514679 144.699661255,116.156349182 172.907104492,157.495025635\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 164.241394043,138.595336914 C 160.649810791,117.082008362 168.714874268,114.288925171 176.743988037,104.940284729\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 159.371765137,95.7409439087 C 195.13722229,100.245635986 224.69380188,134.492462158 230.739959717,169.187133789\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 221.917114258,153.861633301 C 195.628982544,195.038497925 210.012786865,216.327926636 183.361236572,230.869476318\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 161.959472656,224.108673096 C 195.834121704,237.330657959 174.061203003,240.171508789 177.696594238,239.656158447\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 156.195404053,248.846221924 C 208.15020752,239.646179199 248.822616577,208.73097229 239.494369507,175.876586914\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 243.070129395,166.774353027 C 242.012786865,116.916313171 219.480209351,92.0129394531 177.607208252,84.2614974976\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 161.544433594,84.4290237427 C 156.439956665,77.1265869141 166.847137451,90.7735443115 173.840377808,81.9592895508\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 84.1051635742,214.566650391 C 91.2473602295,235.932357788 121.864402771,216.515518188 144.375793457,239.611618042\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 130.280670166,244.402389526 C 123.296318054,236.498519897 133.595947266,241.165130615 146.818054199,235.137191772\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 136.500411987,231.131774902 C 114.200737,224.329299927 108.918144226,218.812179565 105.396697998,221.153717041\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 99.8319854736,220.086364746 C 87.6996612549,211.598327637 96.3422164917,229.408737183 90.8463134766,232.295578003\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 86.2730712891,229.86756897 C 78.6575927734,225.559783936 84.9899978638,230.537658691 90.6410369873,231.467681885\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 74.7118682861,181.558639526 C 58.2888946533,163.444000244 60.0914344788,175.059753418 53.7447624207,184.605712891\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 52.3242874146,180.995574951 C 51.9291381836,171.053207397 61.2986526489,197.552856445 75.2585067749,222.660263062\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 73.4063568115,220.320556641 C 67.2691802979,195.597991943 80.3762130737,202.118179321 89.1727981567,214.480789185\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 85.4886550903,211.259338379 C 78.0659103394,181.797073364 72.9622039795,187.897644043 75.1873245239,184.543304443\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 74.0244445801,182.098403931 C 69.1817855835,159.448974609 71.9711914062,175.878540039 77.3590545654,186.848266602\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "\n",
       "</svg>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"300\" version=\"1.0\" width=\"300\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "     &quot;<path d=\"M 50.0,50.0 C 50.0,50.0 50.0,50.0 50.0,50.0\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 191.115020752,75.9389953613 C 218.564575195,103.72063446 148.526672363,15.0162353516 82.3462295532,108.744041443\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 152.760375977,56.6826820374 C 149.854431152,62.57654953 156.112579346,98.9979171753 104.421005249,97.5242767334\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 108.233200073,101.882278442 C 136.539230347,113.20514679 144.699661255,116.156349182 172.907104492,157.495025635\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 164.241394043,138.595336914 C 160.649810791,117.082008362 168.714874268,114.288925171 176.743988037,104.940284729\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 159.371765137,95.7409439087 C 195.13722229,100.245635986 224.69380188,134.492462158 230.739959717,169.187133789\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 221.917114258,153.861633301 C 195.628982544,195.038497925 210.012786865,216.327926636 183.361236572,230.869476318\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 161.959472656,224.108673096 C 195.834121704,237.330657959 174.061203003,240.171508789 177.696594238,239.656158447\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 156.195404053,248.846221924 C 208.15020752,239.646179199 248.822616577,208.73097229 239.494369507,175.876586914\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 243.070129395,166.774353027 C 242.012786865,116.916313171 219.480209351,92.0129394531 177.607208252,84.2614974976\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 161.544433594,84.4290237427 C 156.439956665,77.1265869141 166.847137451,90.7735443115 173.840377808,81.9592895508\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 84.1051635742,214.566650391 C 91.2473602295,235.932357788 121.864402771,216.515518188 144.375793457,239.611618042\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 130.280670166,244.402389526 C 123.296318054,236.498519897 133.595947266,241.165130615 146.818054199,235.137191772\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 136.500411987,231.131774902 C 114.200737,224.329299927 108.918144226,218.812179565 105.396697998,221.153717041\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 99.8319854736,220.086364746 C 87.6996612549,211.598327637 96.3422164917,229.408737183 90.8463134766,232.295578003\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 86.2730712891,229.86756897 C 78.6575927734,225.559783936 84.9899978638,230.537658691 90.6410369873,231.467681885\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 74.7118682861,181.558639526 C 58.2888946533,163.444000244 60.0914344788,175.059753418 53.7447624207,184.605712891\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 52.3242874146,180.995574951 C 51.9291381836,171.053207397 61.2986526489,197.552856445 75.2585067749,222.660263062\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 73.4063568115,220.320556641 C 67.2691802979,195.597991943 80.3762130737,202.118179321 89.1727981567,214.480789185\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 85.4886550903,211.259338379 C 78.0659103394,181.797073364 72.9622039795,187.897644043 75.1873245239,184.543304443\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 74.0244445801,182.098403931 C 69.1817855835,159.448974609 71.9711914062,175.878540039 77.3590545654,186.848266602\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 86.7898864746,131.857727051 C 80.6647720337,107.994644165 73.2370758057,125.240585327 77.7924880981,121.537651062\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "\n",
       "</svg>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"300\" version=\"1.0\" width=\"300\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "     &quot;<path d=\"M 50.0,50.0 C 50.0,50.0 50.0,50.0 50.0,50.0\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 191.115020752,75.9389953613 C 218.564575195,103.72063446 148.526672363,15.0162353516 82.3462295532,108.744041443\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 152.760375977,56.6826820374 C 149.854431152,62.57654953 156.112579346,98.9979171753 104.421005249,97.5242767334\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 108.233200073,101.882278442 C 136.539230347,113.20514679 144.699661255,116.156349182 172.907104492,157.495025635\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 164.241394043,138.595336914 C 160.649810791,117.082008362 168.714874268,114.288925171 176.743988037,104.940284729\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 159.371765137,95.7409439087 C 195.13722229,100.245635986 224.69380188,134.492462158 230.739959717,169.187133789\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 221.917114258,153.861633301 C 195.628982544,195.038497925 210.012786865,216.327926636 183.361236572,230.869476318\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 161.959472656,224.108673096 C 195.834121704,237.330657959 174.061203003,240.171508789 177.696594238,239.656158447\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 156.195404053,248.846221924 C 208.15020752,239.646179199 248.822616577,208.73097229 239.494369507,175.876586914\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 243.070129395,166.774353027 C 242.012786865,116.916313171 219.480209351,92.0129394531 177.607208252,84.2614974976\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 161.544433594,84.4290237427 C 156.439956665,77.1265869141 166.847137451,90.7735443115 173.840377808,81.9592895508\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 84.1051635742,214.566650391 C 91.2473602295,235.932357788 121.864402771,216.515518188 144.375793457,239.611618042\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 130.280670166,244.402389526 C 123.296318054,236.498519897 133.595947266,241.165130615 146.818054199,235.137191772\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 136.500411987,231.131774902 C 114.200737,224.329299927 108.918144226,218.812179565 105.396697998,221.153717041\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 99.8319854736,220.086364746 C 87.6996612549,211.598327637 96.3422164917,229.408737183 90.8463134766,232.295578003\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 86.2730712891,229.86756897 C 78.6575927734,225.559783936 84.9899978638,230.537658691 90.6410369873,231.467681885\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 74.7118682861,181.558639526 C 58.2888946533,163.444000244 60.0914344788,175.059753418 53.7447624207,184.605712891\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 52.3242874146,180.995574951 C 51.9291381836,171.053207397 61.2986526489,197.552856445 75.2585067749,222.660263062\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 73.4063568115,220.320556641 C 67.2691802979,195.597991943 80.3762130737,202.118179321 89.1727981567,214.480789185\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 85.4886550903,211.259338379 C 78.0659103394,181.797073364 72.9622039795,187.897644043 75.1873245239,184.543304443\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 74.0244445801,182.098403931 C 69.1817855835,159.448974609 71.9711914062,175.878540039 77.3590545654,186.848266602\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 86.7898864746,131.857727051 C 80.6647720337,107.994644165 73.2370758057,125.240585327 77.7924880981,121.537651062\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 73.2918777466,118.516716003 C 61.012928009,105.144622803 53.7219390869,145.925186157 55.0519561768,166.666809082\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "\n",
       "</svg>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"300\" version=\"1.0\" width=\"300\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "     &quot;<path d=\"M 50.0,50.0 C 50.0,50.0 50.0,50.0 50.0,50.0\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 191.115020752,75.9389953613 C 218.564575195,103.72063446 148.526672363,15.0162353516 82.3462295532,108.744041443\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 152.760375977,56.6826820374 C 149.854431152,62.57654953 156.112579346,98.9979171753 104.421005249,97.5242767334\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 108.233200073,101.882278442 C 136.539230347,113.20514679 144.699661255,116.156349182 172.907104492,157.495025635\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 164.241394043,138.595336914 C 160.649810791,117.082008362 168.714874268,114.288925171 176.743988037,104.940284729\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 159.371765137,95.7409439087 C 195.13722229,100.245635986 224.69380188,134.492462158 230.739959717,169.187133789\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 221.917114258,153.861633301 C 195.628982544,195.038497925 210.012786865,216.327926636 183.361236572,230.869476318\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 161.959472656,224.108673096 C 195.834121704,237.330657959 174.061203003,240.171508789 177.696594238,239.656158447\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 156.195404053,248.846221924 C 208.15020752,239.646179199 248.822616577,208.73097229 239.494369507,175.876586914\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 243.070129395,166.774353027 C 242.012786865,116.916313171 219.480209351,92.0129394531 177.607208252,84.2614974976\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 161.544433594,84.4290237427 C 156.439956665,77.1265869141 166.847137451,90.7735443115 173.840377808,81.9592895508\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 84.1051635742,214.566650391 C 91.2473602295,235.932357788 121.864402771,216.515518188 144.375793457,239.611618042\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 130.280670166,244.402389526 C 123.296318054,236.498519897 133.595947266,241.165130615 146.818054199,235.137191772\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 136.500411987,231.131774902 C 114.200737,224.329299927 108.918144226,218.812179565 105.396697998,221.153717041\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 99.8319854736,220.086364746 C 87.6996612549,211.598327637 96.3422164917,229.408737183 90.8463134766,232.295578003\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 86.2730712891,229.86756897 C 78.6575927734,225.559783936 84.9899978638,230.537658691 90.6410369873,231.467681885\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 74.7118682861,181.558639526 C 58.2888946533,163.444000244 60.0914344788,175.059753418 53.7447624207,184.605712891\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 52.3242874146,180.995574951 C 51.9291381836,171.053207397 61.2986526489,197.552856445 75.2585067749,222.660263062\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 73.4063568115,220.320556641 C 67.2691802979,195.597991943 80.3762130737,202.118179321 89.1727981567,214.480789185\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 85.4886550903,211.259338379 C 78.0659103394,181.797073364 72.9622039795,187.897644043 75.1873245239,184.543304443\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 74.0244445801,182.098403931 C 69.1817855835,159.448974609 71.9711914062,175.878540039 77.3590545654,186.848266602\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 86.7898864746,131.857727051 C 80.6647720337,107.994644165 73.2370758057,125.240585327 77.7924880981,121.537651062\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 73.2918777466,118.516716003 C 61.012928009,105.144622803 53.7219390869,145.925186157 55.0519561768,166.666809082\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 56.6749038696,161.145462036 C 58.9025382996,131.601089478 60.7865867615,156.445846558 74.9248275757,167.558929443\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "\n",
       "</svg>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"300\" version=\"1.0\" width=\"300\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "     &quot;<path d=\"M 50.0,50.0 C 50.0,50.0 50.0,50.0 50.0,50.0\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 191.115020752,75.9389953613 C 218.564575195,103.72063446 148.526672363,15.0162353516 82.3462295532,108.744041443\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 152.760375977,56.6826820374 C 149.854431152,62.57654953 156.112579346,98.9979171753 104.421005249,97.5242767334\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 108.233200073,101.882278442 C 136.539230347,113.20514679 144.699661255,116.156349182 172.907104492,157.495025635\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 164.241394043,138.595336914 C 160.649810791,117.082008362 168.714874268,114.288925171 176.743988037,104.940284729\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 159.371765137,95.7409439087 C 195.13722229,100.245635986 224.69380188,134.492462158 230.739959717,169.187133789\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 221.917114258,153.861633301 C 195.628982544,195.038497925 210.012786865,216.327926636 183.361236572,230.869476318\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 161.959472656,224.108673096 C 195.834121704,237.330657959 174.061203003,240.171508789 177.696594238,239.656158447\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 156.195404053,248.846221924 C 208.15020752,239.646179199 248.822616577,208.73097229 239.494369507,175.876586914\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 243.070129395,166.774353027 C 242.012786865,116.916313171 219.480209351,92.0129394531 177.607208252,84.2614974976\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 161.544433594,84.4290237427 C 156.439956665,77.1265869141 166.847137451,90.7735443115 173.840377808,81.9592895508\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 84.1051635742,214.566650391 C 91.2473602295,235.932357788 121.864402771,216.515518188 144.375793457,239.611618042\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 130.280670166,244.402389526 C 123.296318054,236.498519897 133.595947266,241.165130615 146.818054199,235.137191772\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 136.500411987,231.131774902 C 114.200737,224.329299927 108.918144226,218.812179565 105.396697998,221.153717041\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 99.8319854736,220.086364746 C 87.6996612549,211.598327637 96.3422164917,229.408737183 90.8463134766,232.295578003\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 86.2730712891,229.86756897 C 78.6575927734,225.559783936 84.9899978638,230.537658691 90.6410369873,231.467681885\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 74.7118682861,181.558639526 C 58.2888946533,163.444000244 60.0914344788,175.059753418 53.7447624207,184.605712891\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 52.3242874146,180.995574951 C 51.9291381836,171.053207397 61.2986526489,197.552856445 75.2585067749,222.660263062\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 73.4063568115,220.320556641 C 67.2691802979,195.597991943 80.3762130737,202.118179321 89.1727981567,214.480789185\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 85.4886550903,211.259338379 C 78.0659103394,181.797073364 72.9622039795,187.897644043 75.1873245239,184.543304443\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 74.0244445801,182.098403931 C 69.1817855835,159.448974609 71.9711914062,175.878540039 77.3590545654,186.848266602\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 86.7898864746,131.857727051 C 80.6647720337,107.994644165 73.2370758057,125.240585327 77.7924880981,121.537651062\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 73.2918777466,118.516716003 C 61.012928009,105.144622803 53.7219390869,145.925186157 55.0519561768,166.666809082\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 56.6749038696,161.145462036 C 58.9025382996,131.601089478 60.7865867615,156.445846558 74.9248275757,167.558929443\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 71.4184646606,162.436218262 C 70.4948120117,121.196884155 85.4416656494,137.284362793 91.5678253174,132.054504395\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "\n",
       "</svg>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"300\" version=\"1.0\" width=\"300\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "     &quot;<path d=\"M 50.0,50.0 C 50.0,50.0 50.0,50.0 50.0,50.0\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 191.115020752,75.9389953613 C 218.564575195,103.72063446 148.526672363,15.0162353516 82.3462295532,108.744041443\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 152.760375977,56.6826820374 C 149.854431152,62.57654953 156.112579346,98.9979171753 104.421005249,97.5242767334\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 108.233200073,101.882278442 C 136.539230347,113.20514679 144.699661255,116.156349182 172.907104492,157.495025635\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 164.241394043,138.595336914 C 160.649810791,117.082008362 168.714874268,114.288925171 176.743988037,104.940284729\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 159.371765137,95.7409439087 C 195.13722229,100.245635986 224.69380188,134.492462158 230.739959717,169.187133789\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 221.917114258,153.861633301 C 195.628982544,195.038497925 210.012786865,216.327926636 183.361236572,230.869476318\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 161.959472656,224.108673096 C 195.834121704,237.330657959 174.061203003,240.171508789 177.696594238,239.656158447\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 156.195404053,248.846221924 C 208.15020752,239.646179199 248.822616577,208.73097229 239.494369507,175.876586914\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 243.070129395,166.774353027 C 242.012786865,116.916313171 219.480209351,92.0129394531 177.607208252,84.2614974976\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 161.544433594,84.4290237427 C 156.439956665,77.1265869141 166.847137451,90.7735443115 173.840377808,81.9592895508\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 84.1051635742,214.566650391 C 91.2473602295,235.932357788 121.864402771,216.515518188 144.375793457,239.611618042\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 130.280670166,244.402389526 C 123.296318054,236.498519897 133.595947266,241.165130615 146.818054199,235.137191772\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 136.500411987,231.131774902 C 114.200737,224.329299927 108.918144226,218.812179565 105.396697998,221.153717041\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 99.8319854736,220.086364746 C 87.6996612549,211.598327637 96.3422164917,229.408737183 90.8463134766,232.295578003\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 86.2730712891,229.86756897 C 78.6575927734,225.559783936 84.9899978638,230.537658691 90.6410369873,231.467681885\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 74.7118682861,181.558639526 C 58.2888946533,163.444000244 60.0914344788,175.059753418 53.7447624207,184.605712891\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 52.3242874146,180.995574951 C 51.9291381836,171.053207397 61.2986526489,197.552856445 75.2585067749,222.660263062\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 73.4063568115,220.320556641 C 67.2691802979,195.597991943 80.3762130737,202.118179321 89.1727981567,214.480789185\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 85.4886550903,211.259338379 C 78.0659103394,181.797073364 72.9622039795,187.897644043 75.1873245239,184.543304443\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 74.0244445801,182.098403931 C 69.1817855835,159.448974609 71.9711914062,175.878540039 77.3590545654,186.848266602\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 86.7898864746,131.857727051 C 80.6647720337,107.994644165 73.2370758057,125.240585327 77.7924880981,121.537651062\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 73.2918777466,118.516716003 C 61.012928009,105.144622803 53.7219390869,145.925186157 55.0519561768,166.666809082\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 56.6749038696,161.145462036 C 58.9025382996,131.601089478 60.7865867615,156.445846558 74.9248275757,167.558929443\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 71.4184646606,162.436218262 C 70.4948120117,121.196884155 85.4416656494,137.284362793 91.5678253174,132.054504395\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "&quot;<path d=\"M 50.0,50.0 C 50.0,50.0 50.0,50.0 50.0,50.0\" stroke-width=\"5\" style=\"fill:none;stroke:black;\"/>\n",
       "\n",
       "</svg>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import SVG, display\n",
    "from IPython.core.display import display_svg\n",
    "for i in range(1):\n",
    "    #o = outputs_normal[1].copy()\n",
    "    o = generate(capsule, nb_steps=25)\n",
    "    \n",
    "    #o[1:] += o[0:-1]\n",
    "    \n",
    "    #break\n",
    "    for k in range(len(o)):\n",
    "        display_svg(SVG(data=gen_svg_from_output(o[0:k + 1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
